# This is a initial Skype record
polo_ffm, 12:00 PM
EI w/ constraints:
- very limited
- needs noisefree observations
- needs to find at least one feasible point first before it can start the EIc criterion
- obj and constraints are independent

polo_ffm, 12:01 PM
third assumption does not hold in general

polo_ffm, 12:03 PM
PESC: 
- still supposes constraints and obj are independent
- additivity assumption on acq. value
- very complicated acq. func. / contrast this with EI or KG that are much easier to compute in practice

polo_ffm, 12:04 PM
MISO: KG with multiple information source.

That allows to find an optimum much faster by using cheap approximation.

polo_ffm, 12:05 PM
Can we extend misoKG to handle constraints?

Chongke, 12:06 PM
Yes, but does we need to consider the decoupling scenario?

polo_ffm, 12:06 PM
If we can do that, then we have a huge impact in engineering (materials science, aerospace) and machine learning

polo_ffm, 12:07 PM
utility u(x) = tau if x is not feasible, or obj(x) if x is feasibe

polo_ffm, 12:08 PM
E_N is the exp. value of some quantity at time N

polo_ffm, 12:09 PM
E_N[u(x)] = (1 - prob_N[x is feasible])  tau + prob_N[x is feasible]  E_N[ obj(x) | x is feasible]

polo_ffm, 12:10 PM
tau is some known penalty

Chongke, 12:12 PM
Is that concept similar to EIC?

polo_ffm, 12:12 PM
If obj(x) and constr(x) are independent, then E_N[ obj(x) | x is feasible] = E_N[ obj(x) ]

polo_ffm, 12:14 PM
If I am done sampling, I do not get further information and have to recommend a point x now: which one should I pick?

polo_ffm, 12:15 PM
argmax_x E_N[ u(x) ]

polo_ffm, 12:15 PM
now let's make the step towards KG:

polo_ffm, 12:15 PM
suppose that we have one more sample, where should we sample next?

polo_ffm, 12:16 PM
max_{x'} E_n[u(x')] is the best point at time n

polo_ffm, 12:17 PM
max_{x'} E_{n+1}[u(x')] is the best point after making the next obs

polo_ffm, 12:17 PM
let's maximize the difference

Chongke, 12:19 PM
that is exactly concept of KG, right?

polo_ffm, 12:19 PM
max_{x} E_{n}[ max_{x'}E[ u(x') | sample x next] - max_{x'} E_n[u(x')]

polo_ffm, 12:20 PM
yes, this is the KG concept applied to the utility function u(x) rather than to the obj(x) directly

polo_ffm, 12:21 PM
the utility accounts for the feasibility under the constraints, which is not contained in the original KG formulation

Chongke, 12:22 PM
so the utility's definition is obj(x) + feasibility?

polo_ffm, 12:23 PM
E_N[u(x)] = (1 - prob_N[x is feasible])  tau + prob_N[x is feasible]  E_N[ obj(x) | x is feasible]
a bit more complicated, because the feasibility is uncertain

polo_ffm, 12:24 PM
if you read the experimental section of the PESC paper carefully, then you see that there is a notion of a penalty tau for infeasible recommendations

polo_ffm, 12:26 PM
cKG(x) =  E_{n}[ max_{x'}E[ u(x') | sample x next] - max_{x'} E_n[u(x')]

polo_ffm, 12:27 PM
KG(x) becomes cKG(x) by incorporating constraints into the utility

polo_ffm, 12:27 PM
in every step, sample an x \in argmax_x cKG(x)

polo_ffm, 12:28 PM
here we do not take into account multiple IS yet, but we can add them easily afterwards

polo_ffm, 12:28 PM
recall the Bonilla et al paper for multi-task GPs

polo_ffm, 12:29 PM
think of task 0 as the obj(x), and of task 1 as the constraint c(x).

polo_ffm, 12:29 PM
Then we have a kernel on domain X, say K_x

polo_ffm, 12:29 PM
Let K_x be the ARD 5/2 Matern kernel where the signal variance is fixed to 1.0

polo_ffm, 12:30 PM
positive definite matrix K_s which gives the relationship between both tasks

polo_ffm, 12:59 PM
our goal is to find a point of maximum utility

polo_ffm, 1:00 PM
we have turned this into a one-step acquisition, where we make one observation, we update the posterior, and we iterate

polo_ffm, 1:00 PM
cKG(x) =  E_{n}[ max_{x'}E[ u(x') | sample x next] - max_{x'} E_n[u(x')]
the one-step acquisition

polo_ffm, 1:01 PM
we could sample x at the obj, or at the constraint, or at both

polo_ffm, 1:02 PM
let h be the function where we observe.
So h \in {obj, constr1, constr2,...}

polo_ffm, 1:02 PM
cKG(h,x) = E_{n}[ max_{x'}E[ u(x') | sample h(x) next] - max_{x'} E_n[u(x')]

polo_ffm, 1:03 PM
h(x) means sample x at h

polo_ffm, 1:04 PM
we need a covariance between the obj and the constraints. Let's use the covariance of a multitask GP, like the one described in the Bonilla et al paper

polo_ffm, 1:04 PM
Cov( (h,x), (h',x') ) = K_s(h,h') * K_x(x,x')

polo_ffm, 1:05 PM
where K_x is a 5/2 Matern kernel with signal variance set 1.0

polo_ffm, 1:06 PM
K_s is a real, symmetric, positive definite matrix (not a covariance function).

Let us set K_s = L L^T.

L is a lower triangular matrix. It is the Cholesky factor.
We can represent K_s by L, and then we know that L L^T is positive definite as required!

polo_ffm, 1:09 PM
As you said, the generative model is different from the MISO paper.

But the MISO paper only requires us to express

Cov( (h,x), (h',x') )

polo_ffm, 1:11 PM
in misoKG, we wonder how the observation of some x at IS s affects the optimum at IS 0.



polo_ffm, 1:11 PM
Here, we wonder how the observation of some x at some function h (either the obj or some constr) affects the optimum under the obj.

polo_ffm, 1:13 PM
when you look at the MISO paper, you see the formulation of misoKG(s,x)

polo_ffm, 1:14 PM
write the analogous formulation for cKG(h,x).

Here is no cost involved!

polo_ffm, 1:15 PM
you can use this
Cov( (h,x), (h',x') ) = K_s(h,h') * K_x(x,x')

polo_ffm, 1:15 PM
the covariance matrix K = K_s \kronprod K_x

polo_ffm, 1:16 PM
keep in mind that the Cholesky factor of the kronecker product in this case can be written as the kronecker product of the cholesky factors of K_s and K_x

polo_ffm, 1:16 PM
(google for that)

polo_ffm, 1:17 PM
write the analogous formulation for cKG(h,x).

Here is no cost involved!


Chongke, 1:17 PM
okay

Chongke, 1:20 PM
It is very similar

Chongke, 1:20 PM
replace mu with utility 

polo_ffm, 1:21 PM
Suppose that sample x at h and observe Y_1, Y_2, etc.

polo_ffm, 1:21 PM
for each of these observations, you can compute the resulting expected utility

polo_ffm, 1:23 PM
in the MISO, I integrate over all possible observations Y that we can get by sampling h(x).

Here, let us focus first on the case that we take a finite number of MC samples.

In fact, let us start with only one observation Y_1 when sampling h(x)

polo_ffm, 1:24 PM
what is max_x' E_{n+1}[u(x')] if we suppose that have sampled x at h and observed Y_1 ?

Chongke, 1:27 PM
we can still get the posteri expectation curve base on the sampled Y_1

polo_ffm, 1:28 PM
yes, exactly!
we know how the posterior mean and the posterior variance change

polo_ffm, 1:28 PM
that's why we can compute a point of maximum expected utility

Chongke, 1:29 PM
yes in KG is do like this

polo_ffm, 1:29 PM
then we can take a large number of sampled obs Y and average for each x the utility under all these hypothetical realizations

polo_ffm, 1:31 PM
in misoKG we can analytically compute that for all possible observations Y, because the posterior optimum depends only on the mean (the expected obj value).

Here it is more complicated, since the utility depends on the feasibility under the constraint and on the conditional mean under the obj (the expected obj value conditioned on that x is feasible)

polo_ffm, 1:31 PM
but we can approximate it

Chongke, 1:33 PM
change the mean into utility 

Chongke, 1:33 PM
the constrain is still in the form of c(x)>0?

polo_ffm, 1:33 PM
correct

polo_ffm, 1:35 PM
prob_n[x is feasible] is Gaussian.

polo_ffm, 1:36 PM
We know the mean and the variance at x' for function (constraint) h before making the observation.
And we have the update equations that tell us how the mean and the variance at x' change if we observe Y_1 when sampling x at function h.

polo_ffm, 1:37 PM
thus, we can compute what the prob of being feasible is conditioned on that we observe Y_1 when sampling h(x)

Chongke, 1:38 PM
I will send you a summary later

Chongke, 1:40 PM
2 course

Chongke, 1:40 PM
what year

Chongke, 1:41 PM
first year

polo_ffm, 1:41 PM
what are the required courses of your department?

Chongke, 1:41 PM
no exactly as long as I have 3 ECE department courses

Chongke, 1:42 PM
To take qualify exam

polo_ffm, 1:42 PM
machine learning is great.

what about algorithms or optimization?

polo_ffm, 1:42 PM
stochastic processes or statistics or probability theory?

Chongke, 1:42 PM
Looks great

Chongke, 1:43 PM
I don't have a advisor in ECE

Chongke, 1:45 PM
I am TA in ECE


Chongke, 1:45 PM
in December

Chongke, 1:45 PM
Should I take to them?

Chongke, 1:45 PM
talk